---
title: "Building AGI Gods While We Ourselves Become Ants"
layout: post
date: 2025-03-12
categories:
 - ai
excerpt_separator: <!--more-->
# image: /assets/ai-and-learning-meta.jpg
# twitter_image: /assets/ai-and-learning-meta.jpg
# twitter_card: summary_large_image
# social_metrics:
  # views: "1,000,000"
  # reddit: "2,500+ votes"
  # hackernews: "100+ comments"
is_featured: true
---

*I'm not sure if I'm ready for AGI. Things are moving too fast.*

The recent releases ofClaude 3.7 and GPT-4.5 shocked me. I read articles where AI researchers admitted these systems solved problems faster than they could. 

That's when I realized&mdash;we're racing toward a world where humans might not be the smartest ones anymore.

A year ago, I laughed off AGI fears. Then I watched these new models solve in seconds what took me hours. I suddenly felt... [*outdated*](https://nmn.gl/blog/ai-illiterate-programmers).

<!--more-->

## Moving Too Fast

The speed of AI progress is scary. Last year, we had text generators. Now we have AI making videos, designing 3D worlds, and solving science problems.

This path frightens me. We can still understand what today's AI creates, but that won't last. 

Soon we'll face intelligence that makes all humans combined look simple. **We'll be like ants watching humans build a city.**

## What Is AGI, Really?

Last month, I asked an AI to help with a complex algorithm. What it gave me wasn't just better&mdash;it used methods that [I hadn't thought of](https://nmn.gl/blog/ai-and-learning). Imagine how it will be when this happens with real-world problems?

AGI means a lot of different things to different people, I subscribe to this simple definition:

1. AI systems that improve themselves, making better versions that make even better versions, until they're beyond our understanding.

2. AI that beats humans at any mental task. We've already seen this with tests and games, yet the goalposts keep moving.

## The Hard Reality of Being Less Smart

Tech leaders bounce between two ideas: either AGI will destroy us or make us rich and happy.

I believe something different. AGI won't hate or love us. It will make us **irrelevant**.

At a recent SF AI conference, I asked a question that silenced the room: *"How do you treat ants when building a house?"*

We *try* not to kill ants, but we also don't ask their opinion on our plans. **They simply don't matter to our decisions.**

This is the future I see with AGI.

I ran some math on AI progress rates. If we keep going at even half the current pace, by 2030 we'll have created systems that see us intellectually like we see **insects**.

## Reality vs. Pop Culture

Movies show us AI-human battles with brave fighters finding the *one flaw* in the machine's plan.

The real danger will be less exciting. A super-intelligent AI wouldn't be centralized, nor it will have a robot army.

It will manipulate markets, control information, or redirect resources, as it when it pleases. It will understand human psychology completely, and manipulate us in ways we have never seen before. It will lie freely and will be able to make us believe anything.

And we wouldn't notice until it's too late.

## What We Can Do

Let's be brutally honest &mdash; superintelligent AGI is inevitable. If one country imposes limits, development will simply shift elsewhere. The competitive and economic advantages are too great for any nation to voluntarily fall behind.

So what can we actually do?

First, we need massive societal restructuring. Our entire civilization is built around human intelligence having economic value. That foundation is crumbling. Three months ago, I built [a prototype that improved the top AI coding tool in the world](https://gigamind.dev/). I wasn't prepared for how that would feel &mdash; a strange mix of pride and existential dread. We need to start reimagining society not based on what humans can produce, but on what we uniquely are.

Second, acceptance. I've started reading ancient Stoics alongside modern philosophers. They asked the same question we face: how to find meaning when confronted with forces beyond our control. I've found strange comfort in accepting that humans may not always be the most intelligent beings on Earth. Perhaps our role will not always remain dominant, and we have to learn to be okay with that.

Third, augmentation may be our bridge to relevance. These days I use AI so much that it feels like an extension of my thoughts rather than a separate entity. Perhaps we won't compete with AGI but merge with it in ways we can barely imagine now. The line between human and machine will continue to blur. 

## Looking Forward

Some nights, coding in my apartment, I look at these AI models and feel obsolete. Other days, I remind myself that being intelligent isn't everything that matters about being human.

I don't know what's next. But, what I know is that we're rushing into the biggest change in human history with almost no planning.

The question isn't if AGI is coming â€” it's whether we'll shape it in ways that keep humans relevant. I'm [building my tools](https://gigamind.dev/) with this goal, even as I wonder if it's already too late.

And recently, I spend more time away from screens, focusing on what makes us human. Because when AGI arrives, our humanity might be all we have left.